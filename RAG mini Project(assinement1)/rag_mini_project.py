# -*- coding: utf-8 -*-
"""RAG mini Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PkTT_taCc9OWYQspYL9j9-V-qju9zfl

# RAG Assignment: CSV-Based Question Answering System

## Objective
To develop a **Retrieval-Augmented Generation (RAG)** system that can answer questions using a given **CSV dataset**. The system retrieves the most relevant records from the dataset and then generates a final answer based only on the retrieved context.

## Workflow (RAG Pipeline)
**CSV Dataset → Row-to-Text Conversion → Chunking → Embeddings (MiniLM) → Vector Search (FAISS) → Answer Generation (FLAN-T5)**

## Tools / Libraries Used
- **Google Colab** (execution environment)  
- **pandas, numpy** (data handling & preprocessing)  
- **sentence-transformers** (text embeddings)  
- **FAISS** (vector database for retrieval)  
- **transformers** (LLM for response generation)  
- **Gradio (Optional)** (UI for bonus)

> Run the notebook cells in order and upload the CSV file when prompted.
"""

!pip -q install pandas numpy faiss-cpu sentence-transformers accelerate beautifulsoup4 sentencepiece "transformers<5" gradio

import transformers
print("Transformers version:", transformers.__version__)

"""## Upload CSV

"""

from google.colab import files
uploaded = files.upload()   # upload your CSV file
csv_path = list(uploaded.keys())[0]
print("Using file:", csv_path)

"""## Load and clean dataset

"""

import pandas as pd
import numpy as np

df = pd.read_csv(csv_path)

# Basic cleaning
df = df.replace("?", np.nan)
if "Gender" in df.columns:
    df["Gender"] = df["Gender"].replace({"Femal": "Female"})  # typo fix if present

# Convert numeric columns safely (if these exist)
num_cols = ["Age","Salary","Partner_salary","Total_salary","Price","No_of_Dependents"]
for c in num_cols:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Fill missing values
df = df.fillna("Unknown")

print("Shape:", df.shape)
df.head(5)

"""## Convert each row into a text chunk (1 row = 1 chunk)

"""

def row_to_text(r):
    return (
        f"Customer: Age={r['Age']}, Gender={r['Gender']}, Profession={r['Profession']}, "
        f"Marital_status={r['Marital_status']}, Education={r['Education']}, Dependents={r['No_of_Dependents']}. "
        f"Loans: Personal_loan={r['Personal_loan']}, House_loan={r['House_loan']}, Partner_working={r['Partner_working']}. "
        f"Income: Salary={r['Salary']}, Partner_salary={r['Partner_salary']}, Total_salary={r['Total_salary']}. "
        f"Car: Make={r['Make']}, Price={r['Price']}."
    )

row_docs = [row_to_text(row) for _, row in df.iterrows()]
row_meta = [{"type":"row", "row_id": int(i), "make": df.loc[i, "Make"]} for i in range(len(df))]

print("Example row chunk:\n", row_docs[0][:600])
print("Total row chunks:", len(row_docs))

"""## Create summary chunks (Make-wise stats) for better analytical answers

"""

df_stats = df.copy()

# Convert back numeric for stats (Unknown -> NaN)
for c in ["Salary","Partner_salary","Total_salary","Price","Age","No_of_Dependents"]:
    if c in df_stats.columns:
        df_stats[c] = pd.to_numeric(df_stats[c].replace("Unknown", np.nan), errors="coerce")

if "Make" in df_stats.columns:
    df_stats["Make"] = df_stats["Make"].replace("?", np.nan)

def pct_yes(series):
    s = series.astype(str).str.lower().str.strip()
    return round((s == "yes").mean() * 100, 2)

summary_docs, summary_meta = [], []

if "Make" in df_stats.columns:
    for mk, g in df_stats.groupby("Make"):
        if pd.isna(mk):
            continue
        doc = (
            f"SUMMARY Make={mk}: count={len(g)}; "
            f"avg_price={round(g['Price'].mean(),2)}; median_price={round(g['Price'].median(),2)}; "
            f"min_price={round(g['Price'].min(),2)}; max_price={round(g['Price'].max(),2)}; "
            f"avg_total_salary={round(g['Total_salary'].mean(),2)}; avg_age={round(g['Age'].mean(),2)}; "
            f"personal_loan_yes%={pct_yes(g['Personal_loan'])}; house_loan_yes%={pct_yes(g['House_loan'])}."
        )
        summary_docs.append(doc)
        summary_meta.append({"type":"summary", "row_id": None, "make": mk})

docs_all = row_docs + summary_docs
meta_all = row_meta + summary_meta

print("Total docs (rows + summaries):", len(docs_all))
print("Summary docs:", len(summary_docs))
print("Example summary:\n", summary_docs[0] if summary_docs else "No summaries created.")

"""## Create embeddings

"""

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
emb = embed_model.encode(docs_all, convert_to_numpy=True, show_progress_bar=True).astype("float32")

print("Embeddings shape:", emb.shape)

"""## Build FAISS vector index (cosine similarity)

"""

import faiss

faiss.normalize_L2(emb)
index = faiss.IndexFlatIP(emb.shape[1])
index.add(emb)

print("Vectors indexed:", index.ntotal)

"""## Retrieval function (Top-k)

"""

def retrieve(query, top_k=5):
    q = embed_model.encode([query], convert_to_numpy=True).astype("float32")
    faiss.normalize_L2(q)
    scores, ids = index.search(q, top_k)

    results = []
    for rank, idx in enumerate(ids[0]):
        results.append({
            "score": float(scores[0][rank]),
            "text": docs_all[idx],
            "meta": meta_all[idx]
        })
    return results

# Quick test
r = retrieve("average price of SUV", top_k=3)
for i, item in enumerate(r, 1):
    print(f"\n--- Retrieved {i} | score={item['score']:.3f} | meta={item['meta']} ---")
    print(item["text"][:350], "...")

"""## Generator (FLAN-T5) and RAG answer function

"""

from transformers import pipeline

generator = pipeline("text2text-generation", model="google/flan-t5-base")

def rag_answer(query, top_k=5):
    retrieved = retrieve(query, top_k=top_k)

    # Keep prompt small to avoid truncation
    context = "\n\n".join([f"- {r['text'][:800]}" for r in retrieved])

    prompt = (
        "Answer using ONLY the context below. "
        "If the answer is not in the context, say: Not found in the provided dataset.\n\n"
        f"CONTEXT:\n{context}\n\n"
        f"QUESTION: {query}\n\nANSWER:"
    )

    out = generator(prompt, max_new_tokens=220)[0]["generated_text"]
    return out, retrieved

"""## Minimum 3 test queries (Required)

"""

test_queries = [
    "Which car make has the highest average price?",
    "What is the loan pattern for SUV buyers?",
    "Give a typical customer profile for Sedan buyers."
]

for q in test_queries:
    ans, src = rag_answer(q, top_k=5)
    print("\n==============================")
    print("Query:", q)
    print("Answer:", ans)
    print("Top sources (type, make, score):", [(s["meta"].get("type"), s["meta"].get("make"), round(s["score"],3)) for s in src])

"""## Summary
This assignment successfully implements a **Retrieval-Augmented Generation (RAG)** system on a **CSV dataset**. The dataset records are converted into text chunks and stored as embeddings using **MiniLM**. A **FAISS** vector index is used to retrieve the most relevant chunks for each user query, and **FLAN-T5** generates the final answer using only the retrieved context.  
The system was tested using multiple queries and produced grounded, dataset-based responses. Future improvements can include better chunking for tabular data, reranking/hybrid search for higher retrieval accuracy, metadata-based filtering (Make/Profession/Income), and a complete UI using Gradio or Streamlit for end-user interaction.

"""